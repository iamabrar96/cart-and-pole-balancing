{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=2)\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)              # flatten the given tensor first before passing it to fully connected layer\n",
    "        t = F.relu(self.fc1(t))                 # applying an activation functin to fully connected layer 1 and 2\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')       ## namedtuple a python function for creating tuple with names\n",
    ")\n",
    "## Replay Memory (It is place where expirence is stored)\n",
    "\n",
    "class ReplayMemory():                        ## Replaymemory has three functions push,sample,can provide sample\n",
    "    def __init__(self, capacity):            ## capacity is only parameter required in specifying Replaymemory class\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    " # we need to find a way to store experince in the memory                                              \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "class EpsilonGreedyStrategy():                     ## it has one function exploration decay rate\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)\n",
    "class Agent():\n",
    "        def __init__(self, strategy, num_actions, device):\n",
    "            self.current_step = 0\n",
    "            self.strategy = strategy\n",
    "            self.num_actions = num_actions\n",
    "            self.device = device\n",
    "        def select_action(self,state,policy_net):                # policy network is just our Q-network\n",
    "            rate=strategy.get_exploration_rate(self.current_step)\n",
    "            self.current_step+=1\n",
    "\n",
    "            if rate > random.random():\n",
    "                action = random.randrange(self.num_actions)\n",
    "                return torch.tensor([action]).to(self.device)                    # exploration     \n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    return policy_net(state).argmax(dim=1).to(self.device)          # exploitation\n",
    "class CartPoleEnvManager():\n",
    "    def __init__(self, device):\n",
    "        self.device = device                       # it indicates the pytorch to use which device for tensor calculations\n",
    "        self.env = gym.make('CartPole-v0').unwrapped\n",
    "        self.env.reset()             # it indicates that it is at initial stage of the game or starting stage of the episode\n",
    "        self.current_screen = None    #  indication of starting  an episode\n",
    "        self.done = False            # it indicats whether an action taken ended in an episode\n",
    "        \n",
    " # There are few wrapper functions that simply wrap the function with the same name used by gym\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()             # Always the environment has to be  reset to stating state\n",
    "        self.current_screen = None   # when we reset the env we are at the end of the episode ans setting screen to none indicates that we are at start of the episode and havent render the screen\n",
    "        \n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()            # when the episode is finished  we  close the environment\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "    def num_actions_available(self):          # in cart and pole env there are two action left and right\n",
    "        return self.env.action_space.n\n",
    "    def take_action(self, action):        # using the attribute action it calls the step on env to execute the action taken by the agent in the env\n",
    "        _, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    def just_starting(self):                               # starting an Episode\n",
    "        return self.current_screen is None\n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:  # if it just starting or the episode had  ended(in both the cases we set our screen as black screen)\n",
    "\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:                             #when its not starting state or episode has not ended then we are in somewhere middle of the episode\n",
    "\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2  # s2(get_proceesd_screen) it is the current screen,s1 is the last screen(black_screen)\n",
    "            return s2 - s1\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]    # the input tensor has an order as (batch,colour_channel,height,width)\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render('rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    def crop_screen(self, screen):           # it takes screen as attribute for which it needs to be cropped\n",
    "        screen_height = screen.shape[1]      # calling the screen from get_processed_screen to take the height which we need to crop\n",
    "\n",
    "\n",
    "    # Strip off top and bottom\n",
    "        top = int(screen_height * 0.4)    #  setting top as  40% of the screen_height\n",
    "        bottom = int(screen_height * 0.8) #  setting bottom as  80% of the screen_height\n",
    "        screen = screen[:, top:bottom, :] #  taking  a slice strating from top value to bottom value so that we essentially removed 40% from top and 20% from bottom\n",
    "\n",
    "        return screen\n",
    "    def transform_screen_data(self, screen):       \n",
    "    # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 # convert into sacle and rescaling all values by 255 is commom practice for image conversion in ML\n",
    "        screen = torch.from_numpy(screen) # converting into tensor\n",
    "\n",
    "#  torchvision package to compose image transform \n",
    "# torchvision compose class to chain together several image transformation and call thsi resize\n",
    "\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,70))            # the tensor shape after resize is [2,40,70]\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial image without cropping \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em = CartPoleEnvManager(device)\n",
    "em.reset()\n",
    "screen = em.render('rgb_array')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen)\n",
    "plt.title('Non-processed screen example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image with cropped screen\n",
    "screen = em.get_processed_screen()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')   # permute change the order of the tensor [40,70,2]\n",
    "plt.title('Processed screen example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strating state of the screen (it i srepresented a sthe black screen)\n",
    "screen = em.get_state()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
    "plt.title('Starting state example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the state of the screen in between the start and end of the episode\n",
    "for i in range(5):\n",
    "    em.take_action(torch.tensor([1]))\n",
    "screen = em.get_state()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
    "plt.title('Non starting state example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the state of the screeen at the end of the episode (it is a black screen)\n",
    "em.done = True\n",
    "screen = em.get_state()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
    "plt.title('Ending state example')\n",
    "plt.show()\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "    plt.plot(get_moving_average(moving_avg_period, values))\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython: display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "gamma = 1                  # discounted_rate in Bellmann equation\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.01\n",
    "target_update = 10                # the frequency at which we update the target_nett wights abd bias\n",
    "memory_size = 100000              # replay memory size where it store all the experience gained by the agent\n",
    "lr = 0.01\n",
    "num_episodes = 1000             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# calling all the classes \n",
    "em = CartPoleEnvManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "# inserting the input to policy_nett and target_nett\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())    # we set the weights and bias of target_nett as same as policy _nett using python state_dict,load_state_dict functions\n",
    "target_net.eval()              # it indicates that this targetnetwork is only for inferenece not for training\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences)) #states,action,reward,next_state attribute are all set to tuple of containing all the corresponding states,action,reward,next_stae from expirence list\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)             # concatenating extract all the actions from the batch to their own tensor\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the whole idea behind this class (Qvalues) is to find the states location and its value if the state is at final location \n",
    "# then we know tha episode is ended and all the qvalues are zero hence we dont need to send this qvalues to target_nett only the \n",
    "# values of which are in non_final_position we need to send to target q_values by taking the max value of q at that state\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1)) # it returns the final q values for the specific states\n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):      \n",
    "# finds the final state location as we know the final state is when epiosde is ended and it is black screen with all zeros\n",
    "# therefore it searches for the highest q vaue in that state and if the highest value is zero then it is true (bool) \n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False) # it is the opposite of the final_state_location\n",
    "        non_final_states = next_states[non_final_state_locations] # we can get the values of the non_final_state by indexing it to next_state\n",
    "        batch_size = next_states.shape[0] # we check the batch size by looking at how many states are there in our next_state\n",
    "        values = torch.zeros(batch_size).to(QValues.device) # creating a tensor of zeros of batch_size which is equal to size of next_state\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#              Training the loop    \n",
    "\n",
    "episode_durations = []\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()                    # initiallly resetting our environment\n",
    "    state = em.get_state()       # restting the state to initial state or the initial position\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Experience(state, action, next_state, reward)) #storing the experince that the agent gained in the memory using push function\n",
    "        state = next_state\n",
    "        if memory.can_provide_sample(batch_size): # checking if our memory size is atleast equal or greater than batch_size\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences) # extracting all states,action,rewards,next_sate in to tesnor from experince list using extract_tensor function\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1)) # loss function to calculate the loss b/w our current and target qvalue\n",
    "            optimizer.zero_grad() # this function set all the weights and bias to zero in policy nett before we do backprop as in pytorch weights and bias acumulate during backprop\n",
    "            loss.backward()       # computes gradient of the loss \n",
    "            optimizer.step()      # it updates the weights and bias with gradient \n",
    "        if em.done:              # checking if the last action taken by the agent ended the episode\n",
    "            episode_durations.append(timestep) # if it is ended then apppend all the timestep in to episode to see how many timestep it took to complete one episode\n",
    "            plot(episode_durations, 100)        #  plotting with 100 period moving average\n",
    "            break\n",
    "    if episode % target_update == 0: # check whether the  episode is a multiple of 10 as it has set to upadte the weight and bias of target_nett for evevry 10 episode\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "em.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    plotting\n",
    "\n",
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "        moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.random.rand(400), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
